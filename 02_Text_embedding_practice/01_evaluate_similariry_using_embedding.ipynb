{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [re.sub(r\"[.',]\", \"\", sentence).lower().split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer',\n",
       " 'simpson',\n",
       " 'forgot',\n",
       " 'his',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'home',\n",
       " 'so',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'burger',\n",
       " 'on',\n",
       " 'his',\n",
       " 'way',\n",
       " 'to',\n",
       " 'work']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer 의 vector representation: \n",
      "[-1.66068680e-03 -4.20303462e-04  1.09255337e-03 -2.18484551e-03\n",
      " -3.23499087e-03 -3.12648294e-03  3.04949097e-03  1.87829603e-03\n",
      " -1.61115616e-03 -2.80928891e-03  4.52324020e-04  9.58630932e-04\n",
      " -4.38321120e-04  4.20039461e-04 -1.44900673e-03  1.57583959e-03\n",
      "  5.10560756e-04  2.96067167e-03 -3.31212161e-03 -1.82445510e-03\n",
      " -3.01285670e-03 -1.11729649e-04 -2.61634029e-03  1.71959959e-03\n",
      " -2.11100094e-03 -2.03485345e-03  1.69092277e-03 -2.72677210e-03\n",
      "  4.76851710e-04 -2.43410910e-03  3.28659941e-03  2.89322482e-03\n",
      "  5.95899648e-04  1.94471062e-03  1.52419391e-03 -1.98301394e-03\n",
      "  3.28957522e-03 -3.26667982e-03  2.68764189e-03  9.17330268e-04\n",
      " -9.88418004e-04 -1.19753159e-03  3.04961996e-03 -1.81994250e-03\n",
      "  2.79433304e-03 -1.95553945e-03  2.79207341e-03 -1.48160951e-04\n",
      "  2.66201468e-03 -1.02206389e-03  2.01950571e-03  2.95562134e-03\n",
      "  8.11300066e-04  4.55215428e-04  1.65783032e-03  2.71425396e-03\n",
      "  2.85594515e-03  2.84643611e-03  2.36571440e-03  2.69569410e-03\n",
      "  2.86536408e-03  1.06979742e-05 -3.44244880e-04  5.48140437e-04\n",
      " -5.46397168e-05  2.81422050e-04 -2.87100067e-03 -3.18586407e-03\n",
      " -7.79178401e-04  2.93133804e-03 -1.23406888e-03 -2.29325425e-03\n",
      "  1.63925881e-03  3.19730287e-04  6.20175560e-04  1.22176390e-03\n",
      "  1.14811910e-03  1.92712888e-03  4.05051280e-04  3.37304111e-04\n",
      "  3.05107539e-03  9.27790359e-04 -1.60060322e-03  2.26521911e-03\n",
      "  1.76511216e-03  9.88276559e-04 -1.08301686e-03  1.13988691e-03\n",
      "  2.16252939e-03  2.34275823e-03  3.24678927e-04 -2.81540281e-03\n",
      "  7.21826509e-05  1.40061165e-04  1.34546531e-03 -3.10492166e-03\n",
      "  3.28488904e-03 -2.38402886e-03  1.96512206e-03 -3.11573036e-03\n",
      "  3.24020907e-03  2.45439238e-03  4.50846332e-04 -1.12356886e-03\n",
      " -1.32229543e-04  1.22046615e-04 -2.09888932e-03  1.93005183e-03\n",
      "  7.24676589e-04  1.26012217e-03 -2.42453138e-03  2.82842526e-03\n",
      "  1.41696830e-04 -8.03092407e-05 -3.03274323e-03  1.35573710e-03\n",
      "  2.25264905e-03  2.45702779e-03 -2.15250649e-03 -2.68144207e-03\n",
      " -1.84272008e-03 -1.78062328e-04 -2.73930421e-03 -2.76312162e-03\n",
      " -6.49262918e-04  3.79283010e-04 -3.19513585e-03 -1.26098550e-03\n",
      "  2.15646302e-04  2.32684077e-03  5.77375351e-04 -1.92653519e-04\n",
      " -2.47276342e-03 -2.28028302e-03 -2.15097447e-04  2.50997022e-03\n",
      "  1.80709839e-03 -5.11256221e-04  3.89818801e-04 -3.23218526e-03\n",
      " -4.55559610e-04 -1.58451556e-03  1.93722581e-03 -7.54477456e-04\n",
      " -1.55739812e-03 -3.19571234e-03 -4.31633816e-04 -2.40551936e-03\n",
      " -5.34374383e-04 -1.37561746e-03 -7.84670701e-04 -1.12861244e-03\n",
      " -2.72948877e-03 -4.06166451e-04  5.73715777e-04 -1.33450981e-03\n",
      " -2.56841327e-03 -1.21285731e-03 -3.02833086e-03 -1.85324418e-04\n",
      "  1.97609677e-03 -9.44438973e-04  1.04032643e-03  1.68447522e-03\n",
      "  2.78418534e-03  1.86675217e-03  3.20062647e-03 -3.20460391e-03\n",
      " -2.68438552e-03 -2.21391930e-03 -2.53441045e-03 -2.67933286e-03\n",
      " -2.59062601e-03 -9.35797405e-04  4.70210012e-04 -9.60970414e-04\n",
      " -2.90283281e-03  1.63133163e-03  3.08230374e-04  1.53027149e-03\n",
      "  2.39912583e-03  2.56149494e-03 -2.97559076e-04  1.21046661e-03\n",
      " -1.73652580e-03  6.26456167e-04  1.51195272e-03  3.34489834e-03\n",
      " -1.06545002e-03  9.09453316e-04 -1.90982758e-03 -7.30646891e-04\n",
      "  2.71163764e-03 -1.30748888e-03 -3.96405841e-04 -3.12943314e-03\n",
      " -3.15011223e-03  2.96484376e-03 -1.89172395e-03  1.65633566e-03\n",
      " -2.33992934e-03 -8.17773107e-04 -2.67595844e-03  2.52207834e-03\n",
      "  2.07958114e-03  1.73536746e-03  2.80222762e-03 -2.07082237e-04\n",
      " -3.09491903e-03  3.03551811e-03 -1.69026875e-03  2.57623312e-03\n",
      "  1.81319134e-03 -3.93923052e-04 -2.51527084e-03 -5.22290589e-04\n",
      "  2.01459555e-03 -2.37731100e-03  4.18180018e-04 -2.67173676e-03\n",
      "  2.90240673e-03 -9.63921542e-04  3.13616078e-03 -1.91106240e-03\n",
      " -3.27868620e-03 -2.89353030e-03 -1.34260673e-03  1.56894093e-03\n",
      " -6.50069196e-05  3.04524880e-03  1.07473449e-03  1.23249064e-03\n",
      "  1.00344489e-03  2.73522572e-03 -8.72792734e-04  2.49517243e-03\n",
      " -3.16875358e-03  9.50374117e-04 -2.13087042e-04  8.35450919e-05\n",
      "  2.30753538e-03 -9.51145252e-04 -8.14512488e-04  1.94224140e-05\n",
      " -1.65353616e-04 -1.20843516e-03  2.11570039e-03 -2.22024950e-03\n",
      "  2.63734371e-03 -1.30435174e-05  8.47343297e-04  1.09062018e-03\n",
      " -7.62929703e-05  5.49671764e-04 -1.08596124e-03  1.55842700e-03\n",
      "  9.04466433e-05 -1.12329691e-03 -2.93511851e-03 -3.35479411e-03\n",
      "  1.13948714e-04 -1.90499390e-03 -3.52673727e-04 -1.46952574e-03\n",
      " -2.92333635e-03  3.45982437e-04  2.00314121e-03 -7.07993808e-04\n",
      " -2.44983658e-03  1.04414253e-03 -1.54392284e-04 -1.81977404e-03\n",
      " -3.66952881e-04 -1.86574136e-04 -1.04066113e-03 -3.27943615e-03\n",
      "  2.55812518e-03  1.24969310e-03 -8.64422240e-04  2.39443034e-03\n",
      "  1.55546571e-04  2.37179967e-03 -5.29629004e-04  2.50017340e-03\n",
      " -1.58586972e-05 -1.98490988e-03 -1.60756963e-03  3.23407492e-03\n",
      "  1.77424648e-04  2.97734747e-04  2.80875596e-03 -2.06154329e-03\n",
      " -5.77152066e-04 -2.73376517e-03 -2.23413017e-03 -2.84535927e-03\n",
      "  1.31321000e-03  9.17847501e-04  1.90934294e-03  8.00744805e-04]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} 의 vector representation: \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.14081521332263947),\n",
       " ('offered', 0.13243569433689117),\n",
       " ('games', 0.12250109761953354),\n",
       " ('her', 0.11486154049634933),\n",
       " ('nuclear', 0.10569247603416443),\n",
       " ('do', 0.09913021326065063),\n",
       " ('toys', 0.0984482690691948),\n",
       " ('office', 0.0924413651227951),\n",
       " ('bart', 0.09009940177202225),\n",
       " ('way', 0.08802291005849838)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar('homer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 유사도 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "marge_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a: np.ndarray, vector_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터간 cosine similarity를 계산\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector_a : np.ndarray\n",
    "        The first input vector.\n",
    "    vector_b : np.ndarray\n",
    "        The second input vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cosine similarity between `vector_a` and `vector_b`, which is a value between -1 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    simillarity = dot_product / (norm_a * norm_b)\n",
    "    return simillarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14081518"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(homer_vector, marge_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\삼성\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\삼성\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and remove the stopwords and non-alphabetic characters for each line of dialogue\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    \"\"\"\n",
    "    Cleans a spaCy Doc object by lemmatizing its tokens and removing stop words,\n",
    "    then joins the remaining tokens into a single string if there are more than two tokens left.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        A spaCy Doc object containing the processed text.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Optional : str\n",
    "        A string composed of the lemmatized, non-stop tokens separated by spaces,\n",
    "        if the resulting list of tokens has more than two elements. Otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z]+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah s pretty feeling station'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89372, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89372"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah', 's', 'pretty', 'feeling', 'station']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |      \n",
      " |      Once you're finished training a model (=no more updates, only querying)\n",
      " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      " |      to reduce memory.\n",
      " |      \n",
      " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |      \n",
      " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, hierarchical softmax will not be used for model training.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If 0, negative sampling will not be used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, rethrow=False, **kwargs)\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- vector_size : 단어 벡터의 차원 수\n",
    "- min_count : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- sg : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21260182, 60280100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module gensim.models.word2vec:\n",
      "\n",
      "train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs) method of gensim.models.word2vec.Word2Vec instance\n",
      "    Update the model's neural weights from a sequence of sentences.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "    progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "    raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "    that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "    you can simply use `total_examples=self.corpus_count`.\n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "    explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "    where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    corpus_iterable : iterable of list of str\n",
      "        The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      "        consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      "        See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "        or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "        See also the `tutorial on data streaming in Python\n",
      "        <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "    corpus_file : str, optional\n",
      "        Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "        You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "        `corpus_file` arguments need to be passed (not both of them).\n",
      "    total_examples : int\n",
      "        Count of sentences.\n",
      "    total_words : int\n",
      "        Count of raw words in sentences.\n",
      "    epochs : int\n",
      "        Number of iterations (epochs) over the corpus.\n",
      "    start_alpha : float, optional\n",
      "        Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "        for this one call to`train()`.\n",
      "        Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "        (not recommended).\n",
      "    end_alpha : float, optional\n",
      "        Final learning rate. Drops linearly from `start_alpha`.\n",
      "        If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "        Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "        (not recommended).\n",
      "    word_count : int, optional\n",
      "        Count of words already trained. Set this to 0 for the usual\n",
      "        case of training on all words in sentences.\n",
      "    queue_factor : int, optional\n",
      "        Multiplier for size of queue (number of workers * queue_factor).\n",
      "    report_delay : float, optional\n",
      "        Seconds to wait before reporting progress.\n",
      "    compute_loss: bool, optional\n",
      "        If True, computes and stores loss value which can be retrieved using\n",
      "        :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "    callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "        Sequence of callbacks to be executed at specific stages during training.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "        >>>\n",
      "        >>> model = Word2Vec(min_count=1)\n",
      "        >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "        >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      "        (1, 30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_mean_vector',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_for_all',\n",
       " 'vectors_lockf',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method most_similar in module gensim.models.keyedvectors:\n",
      "\n",
      "most_similar(positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None) method of gensim.models.keyedvectors.KeyedVectors instance\n",
      "    Find the top-N most similar keys.\n",
      "    Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      "    \n",
      "    This method computes cosine similarity between a simple mean of the projection\n",
      "    weight vectors of the given keys and the vectors for each key in the model.\n",
      "    The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      "    word2vec implementation.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      "        List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      "    negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      "        List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      "    topn : int or None, optional\n",
      "        Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      "        then similarities for all keys are returned.\n",
      "    clip_start : int\n",
      "        Start clipping index.\n",
      "    clip_end : int\n",
      "        End clipping index.\n",
      "    restrict_vocab : int, optional\n",
      "        Optional integer which limits the range of vectors which\n",
      "        are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      "        only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      "        meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      "        specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list of (str, float) or numpy.array\n",
      "        When `topn` is int, a sequence of (key, similarity) is returned.\n",
      "        When `topn` is None, then similarities for all keys are returned as a\n",
      "        one-dimensional numpy array with the size of the vocabulary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method similarity in module gensim.models.keyedvectors:\n",
      "\n",
      "similarity(w1, w2) method of gensim.models.keyedvectors.KeyedVectors instance\n",
      "    Compute cosine similarity between two keys.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    w1 : str\n",
      "        Input key.\n",
      "    w2 : str\n",
      "        Input key.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    float\n",
      "        Cosine similarity between `w1` and `w2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.6219091415405273),\n",
       " ('simpson', 0.5833418369293213),\n",
       " ('son', 0.5303625464439392),\n",
       " ('m', 0.5297139286994934),\n",
       " ('mr', 0.45622143149375916),\n",
       " ('way', 0.4336795210838318),\n",
       " ('maybe', 0.4258110225200653),\n",
       " ('ve', 0.42477402091026306),\n",
       " ('friend', 0.4154490828514099),\n",
       " ('bart', 0.41092047095298767)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['homer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7192193865776062),\n",
       " ('milhouse', 0.5457488894462585),\n",
       " ('mom', 0.5421731472015381),\n",
       " ('boy', 0.5403572916984558),\n",
       " ('m', 0.5353553891181946),\n",
       " ('dad', 0.5148656964302063),\n",
       " ('ll', 0.4800121784210205),\n",
       " ('maggie', 0.47311967611312866),\n",
       " ('kid', 0.4630797505378723),\n",
       " ('school', 0.46018898487091064)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['bart'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Woman : homer = ___ : marge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.5459743738174438),\n",
       " ('simpson', 0.41761404275894165),\n",
       " ('people', 0.4040578305721283)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'homer'], negative=['marge'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.6235493421554565),\n",
       " ('hoover', 0.5296967625617981),\n",
       " ('mom', 0.5210390686988831)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'bart'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bard', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marge'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61653596, -0.6019891 , -0.94934744,  0.28376544,  1.1348646 ,\n",
       "        0.26321322, -0.23323672, -0.27753466, -0.41983634,  1.7742176 ,\n",
       "       -1.296568  , -0.3656422 ,  0.2235764 ,  0.9179288 ,  0.4883457 ,\n",
       "        0.8312497 ,  1.0029693 , -0.6748741 , -1.6038657 , -0.7150788 ,\n",
       "       -0.94990605, -0.2480318 , -0.7942151 ,  0.27103478,  1.2512724 ,\n",
       "        0.12173048,  0.2943026 , -1.7506382 ,  0.7010823 , -0.4782461 ,\n",
       "       -0.9024575 , -0.55438185,  0.43307015,  0.05748967, -1.303301  ,\n",
       "       -0.01815479,  0.50058013, -0.5830111 ,  0.5908915 , -1.6770072 ,\n",
       "        0.01647623,  0.48206076,  0.90807897, -0.6568238 , -0.5112265 ,\n",
       "       -0.24112633, -1.2961593 ,  0.7980117 ,  0.47578752,  0.2899129 ,\n",
       "        0.46487457,  0.8924679 , -1.8701609 , -0.6965123 ,  0.19850846,\n",
       "       -0.01144549,  0.06661145,  1.1753205 ,  0.66201144,  0.64105207,\n",
       "       -0.80141616, -0.58304787, -2.07443   ,  1.1676633 , -0.4093226 ,\n",
       "       -1.0685458 , -0.8839352 , -0.19339511, -1.0874537 , -0.14502606,\n",
       "        1.4541007 ,  0.17493287, -0.17547749, -0.1453369 , -0.5755706 ,\n",
       "        0.8959401 ,  0.9296767 , -0.7503762 ,  1.070748  , -0.30047372,\n",
       "       -1.4641933 ,  0.02493682, -0.32284358,  1.421594  , -0.00303567,\n",
       "       -0.04482555,  1.2554846 ,  1.1659981 ,  1.0175697 ,  0.515053  ,\n",
       "       -1.9367356 , -0.51151836,  0.2800811 ,  0.5330798 , -1.731115  ,\n",
       "        0.8865755 , -0.8678088 , -0.04045575,  0.52365804,  1.485053  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 사용하는 모든 단어는 context에 따라 의미가 다르다\n",
    "- 단어 embedding의 경우 이런 유연성을 확보하지 못 함\n",
    "   - 배를 깎아 먹었다 / 배가 고프다 / 배 멀미를 하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt')\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 14140,  2769,  2012,  1996,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_ids : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- token_type_ids : 문장 번호\n",
    "- attention_mask : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'bank' in sentence 1: tensor([ 7.5762e-01, -4.9297e-01, -1.4577e-01,  3.7644e-01,  1.3760e-01,\n",
      "        -1.1604e-01,  1.9647e-01,  8.5373e-01,  2.8922e-01, -7.7167e-01,\n",
      "         7.0191e-01,  1.5900e-02, -8.7243e-02, -8.7655e-02,  7.8980e-03,\n",
      "        -3.0902e-01,  8.2599e-01, -5.8779e-01,  4.4748e-01, -1.5286e-01,\n",
      "        -3.8126e-01, -3.2590e-03, -1.5456e-01,  8.0130e-01,  4.3496e-01,\n",
      "         3.9089e-01, -6.2273e-02,  4.3417e-01, -5.5404e-01, -6.3325e-01,\n",
      "         9.7596e-02,  1.3885e-01, -1.1386e+00, -2.9132e-01,  1.2839e-01,\n",
      "         1.4953e-01,  2.3165e-01, -4.7182e-01, -1.1090e+00,  2.2412e-01,\n",
      "        -9.2482e-01, -1.0203e-01,  6.8692e-01, -6.2060e-01,  4.4294e-01,\n",
      "        -3.7882e-01,  9.6047e-01, -3.6463e-01,  1.4609e-02, -1.1666e+00,\n",
      "        -4.4949e-01, -2.8636e-01, -2.2656e-02,  1.5022e-01, -2.7032e-01,\n",
      "         1.5229e+00, -1.6890e-01, -5.8337e-01, -7.3641e-01,  3.8574e-01,\n",
      "         8.4156e-02, -4.8477e-01,  8.2392e-01, -2.3315e-01,  2.4931e-01,\n",
      "         6.1016e-01,  8.4325e-01,  8.1627e-01, -6.0537e-01,  2.3150e-01,\n",
      "        -7.1477e-01, -3.7946e-01, -8.8704e-03, -8.4901e-01, -6.3998e-01,\n",
      "         3.6765e-01, -8.4485e-02,  5.2649e-02, -4.7899e-01,  1.0457e-01,\n",
      "        -2.8119e-01,  9.6048e-01, -7.1094e-01,  4.1873e-01,  2.3970e-01,\n",
      "         3.4044e-01,  4.1498e-02, -4.0391e-01, -1.7933e-01,  4.2658e-01,\n",
      "        -1.1609e-01,  2.3952e-01, -4.3662e-01,  2.6793e-01, -1.4744e-01,\n",
      "        -2.4256e-01, -3.7059e-01, -6.7071e-01,  5.0985e-02,  8.5985e-01,\n",
      "         1.9979e-01, -1.1235e+00, -4.1960e-01,  3.2385e-01,  3.7310e-01,\n",
      "         2.1296e-01, -2.0291e-01, -1.1579e-01, -5.1451e-01,  3.0157e-01,\n",
      "         4.8754e-02, -5.2070e-01, -3.0757e-02, -2.9737e-01,  5.1739e-01,\n",
      "         1.3035e-02, -3.5368e-01, -3.1972e-01, -1.5364e-02, -1.2488e-01,\n",
      "         2.7322e-02,  9.7320e-02, -1.3795e-01,  4.7586e-01, -5.0568e-01,\n",
      "         8.4013e-01, -5.2595e-01,  1.1111e-01, -2.6942e-01, -8.9071e-01,\n",
      "         8.6008e-01,  4.2303e-01,  7.5591e-01, -7.2805e-01,  2.4798e-01,\n",
      "         1.3469e-01,  2.2323e-01,  2.4817e-01, -1.0493e+00,  4.1784e-02,\n",
      "         2.9945e-01,  1.1844e-01,  8.1684e-01, -1.9340e-01,  2.3970e-01,\n",
      "        -7.3054e-02,  4.5176e-01, -1.0316e+00,  4.9771e-01,  5.3913e-01,\n",
      "        -7.8097e-02,  4.3941e-02, -4.9300e-01, -3.1500e-01, -2.7602e-01,\n",
      "        -1.6371e-01,  1.4811e-01,  8.2903e-01,  5.0374e-01,  1.2071e-01,\n",
      "         1.1389e-01, -2.4576e-01,  2.8194e-02,  1.8710e-01, -4.6589e-01,\n",
      "        -1.5319e-02,  1.3686e-01,  7.5025e-01,  4.6258e-01,  1.5065e-01,\n",
      "         1.0943e-01, -3.7266e-01,  1.2962e+00,  2.8650e-01, -3.0625e-01,\n",
      "        -5.8469e-01, -5.9800e-01,  2.9231e-03,  2.9662e-01,  4.3081e-01,\n",
      "        -1.7798e-01, -1.4842e-01, -8.1517e-02, -1.2719e-01,  4.9325e-01,\n",
      "        -1.6009e-02,  5.4834e-01,  2.9491e-01,  3.6264e-01,  3.9807e-01,\n",
      "         3.0092e-01, -4.5430e-01, -5.3896e-01, -1.6673e-01,  1.3940e-01,\n",
      "        -4.7732e-01, -6.0494e-01, -4.0824e-01, -4.3003e-01,  3.4086e-01,\n",
      "         3.0817e-01, -4.7235e-02,  3.9145e-01, -3.2526e-01,  7.1497e-02,\n",
      "         7.0374e-02,  8.2706e-02,  2.1829e-01,  4.0616e-01,  4.9473e-01,\n",
      "        -2.6070e-01,  3.8687e-01,  1.3621e-01, -3.1132e-01, -1.0569e-01,\n",
      "         2.3234e-01, -9.8928e-02, -7.1990e-01,  2.6635e-01,  4.1256e-01,\n",
      "        -3.7289e-02,  4.3687e-01, -6.1829e-01,  4.2423e-01,  4.2349e-02,\n",
      "         1.3467e+00, -1.6563e-01, -2.5058e-01, -8.6367e-01,  6.9355e-01,\n",
      "        -9.6454e-02, -1.1618e+00,  2.8117e-02,  5.0324e-01, -3.6122e-01,\n",
      "        -3.4086e-01, -1.0216e+00, -2.6282e-01,  2.2662e-01, -3.7111e-02,\n",
      "         1.7607e-01, -5.6725e-01, -1.6679e-02,  2.6478e-01,  1.9863e-01,\n",
      "         4.8125e-01, -6.5380e-02,  8.7101e-02,  2.2750e-01, -7.2392e-01,\n",
      "        -5.6888e-01, -1.3929e+00, -2.9526e-02, -1.1252e+00, -2.6058e-01,\n",
      "        -2.3528e-01,  1.4737e-01,  1.9319e-01,  1.9312e-02,  6.5178e-01,\n",
      "        -2.2653e-01,  1.2390e-01, -6.9039e-01,  3.0363e-01,  2.7393e-01,\n",
      "        -1.6551e-01,  9.0606e-01,  4.8616e-01, -8.8311e-02, -1.1896e-01,\n",
      "         1.8691e-01, -1.0104e+00,  3.3411e-01,  7.1757e-01, -1.9596e-01,\n",
      "        -5.0364e-01,  8.3742e-01,  3.4842e-01, -1.6814e-01, -7.3969e-01,\n",
      "         3.7322e-01,  5.0120e-01, -1.0611e+00,  4.5506e-02, -4.7678e-02,\n",
      "        -9.6558e-02, -3.5962e-01, -1.3155e-01, -9.6921e-01, -4.5450e-01,\n",
      "        -3.5404e-01,  7.0383e-01, -5.1422e-01, -4.7137e-01, -1.2399e-01,\n",
      "        -3.9355e-01,  5.6745e-01,  5.0884e-01, -2.9644e-01, -4.3583e-01,\n",
      "         3.9691e-01, -3.3781e-01,  4.6041e-01, -1.7909e-01,  2.1426e-01,\n",
      "        -2.5106e-01, -8.3359e-01, -5.0911e-01, -4.8353e+00,  5.7183e-01,\n",
      "        -3.5536e-01, -6.2578e-01,  5.2406e-01,  4.0936e-01, -5.3386e-01,\n",
      "        -4.2480e-01, -3.2633e-01, -7.9705e-01, -7.9134e-01, -2.3688e-01,\n",
      "        -1.6251e-01, -1.2794e-01,  5.3143e-01, -2.7820e-01,  2.3769e-01,\n",
      "        -7.4595e-01,  2.2753e-01,  9.2413e-01, -2.2054e-01, -3.2983e-01,\n",
      "        -8.5897e-02,  3.5949e-02,  1.7318e-01,  1.0800e+00,  2.4229e-02,\n",
      "         4.9093e-01, -4.7244e-01,  1.7040e-01,  1.0067e-01, -1.1054e-01,\n",
      "         1.9138e-01,  7.4046e-02,  2.8187e-01, -2.7840e-01,  1.3712e-01,\n",
      "        -1.0420e-01,  3.9584e-01, -4.5165e-01, -3.3045e-01,  3.7303e-01,\n",
      "         7.7780e-01,  4.9228e-01,  1.5294e+00,  8.4431e-01, -2.3482e-01,\n",
      "        -8.5542e-01, -4.3610e-01,  3.7315e-01,  4.2895e-01, -3.9447e-01,\n",
      "        -1.7558e-01, -1.0060e-01, -5.3039e-01, -1.7911e-01,  5.7789e-02,\n",
      "        -2.5658e-01, -2.4087e-01, -7.5651e-01, -1.0140e-01, -7.4213e-01,\n",
      "        -2.8504e-01, -5.7259e-02,  7.5670e-02, -5.1657e-01, -2.3943e-01,\n",
      "        -1.3733e-01,  5.2394e-01,  3.6555e-01,  1.9676e-01,  4.0457e-01,\n",
      "        -1.1014e+00, -1.1646e+00, -6.1667e-01,  2.9461e-01,  2.7539e-01,\n",
      "        -3.9307e-01,  4.7916e-01, -2.1361e-01, -5.8670e-01, -1.4885e-01,\n",
      "         1.6313e-01, -5.1326e-01,  2.0569e-01, -5.9793e-01, -1.0760e-01,\n",
      "        -2.3753e-01, -2.7480e-01, -8.4008e-01,  8.1154e-02,  2.7223e-01,\n",
      "         5.5606e-01,  3.2172e-01, -3.6429e-01,  3.0844e-01, -3.6528e-02,\n",
      "        -8.6087e-01, -3.8201e-02,  5.0665e-01, -1.8877e-01,  3.6571e-01,\n",
      "        -2.6334e-02, -9.8532e-02,  2.4268e-01, -3.9641e-02, -5.9441e-01,\n",
      "         4.7348e-01, -1.7928e-01,  8.8011e-02,  1.5121e-01, -1.1765e+00,\n",
      "         1.4880e+00, -5.5590e-01, -2.9700e-01,  4.2152e-03,  1.1490e+00,\n",
      "         4.0406e-02, -2.1772e-01,  6.1360e-01,  4.4226e-01,  8.6945e-01,\n",
      "        -1.9688e-01,  1.0166e-02, -3.8633e-01, -3.1426e-01,  5.3050e-01,\n",
      "        -6.1777e-01, -6.7887e-01, -4.8055e-01,  1.0112e-01, -4.4613e-01,\n",
      "        -6.2226e-01, -8.4300e-02, -4.1428e-02,  3.8610e-01, -3.8447e-01,\n",
      "        -2.2768e-01,  1.0555e-01,  3.8706e-01,  5.3173e-03,  8.0021e-01,\n",
      "         1.6192e-01, -1.7679e-01, -1.6308e-01,  4.1770e-02,  2.0427e-01,\n",
      "        -1.9485e-01,  5.7493e-02, -1.0499e-01, -3.3195e-01, -3.4769e-01,\n",
      "         4.5147e-01,  6.0760e-02, -3.7001e-01, -2.6736e-01,  5.8158e-01,\n",
      "        -6.0154e-02, -7.5156e-01,  9.5966e-02, -4.1317e-01,  1.8641e-02,\n",
      "         4.4670e-01, -3.6429e-02, -3.3687e-01,  1.1300e-01, -6.1826e-01,\n",
      "        -4.4915e-01, -5.4280e-02,  8.3114e-01, -1.3385e-01, -3.6786e-01,\n",
      "        -1.7765e-01, -1.8782e-01,  5.8694e-01,  5.8965e-01,  3.3103e-02,\n",
      "        -4.9250e-01,  4.0811e-02, -2.5690e-01, -3.3122e-01, -6.1282e-01,\n",
      "         2.0367e-02, -1.1310e+00,  1.2153e+00,  2.6652e-01, -1.5336e-01,\n",
      "         3.5948e-01,  1.2845e-01,  3.0239e-01, -4.7777e-02,  7.8299e-01,\n",
      "        -1.1420e+00, -2.5311e-01, -9.9050e-01, -5.9923e-01,  2.2070e-01,\n",
      "         4.8868e-02, -4.9378e-01,  7.1134e-01,  4.1387e-01, -4.2784e-02,\n",
      "         6.1836e-02, -3.0679e-01,  1.7834e-01, -2.1941e-01,  3.4909e-01,\n",
      "        -1.2923e-01, -1.4040e-01,  5.4183e-01, -2.9687e-01, -2.1360e-01,\n",
      "         2.6722e-01, -7.5442e-01, -1.4120e-01, -1.6369e-01,  5.5258e-01,\n",
      "         1.7755e-01, -3.0055e-02,  7.9672e-02, -5.3299e-01,  1.0577e-01,\n",
      "        -1.7060e-01,  4.1235e-01, -3.9943e-01,  2.3409e-04,  2.6075e-01,\n",
      "        -2.8518e-01, -3.7892e-01,  8.2828e-02, -4.4226e-01, -6.8398e-01,\n",
      "         5.8223e-01,  2.1746e-01,  1.0238e-01,  6.9105e-01,  1.9621e-02,\n",
      "        -9.5682e-01, -1.2656e-01,  8.9794e-01,  8.4272e-02,  1.1077e-02,\n",
      "         3.8610e-01,  2.7072e-01,  6.2592e-01, -6.8609e-01,  2.8917e-01,\n",
      "         5.6236e-01,  4.2472e-01,  3.1323e-01,  3.1373e-01,  2.4719e-01,\n",
      "         3.8469e-01,  9.9484e-02, -2.5937e-01, -5.0741e-01,  1.0050e-01,\n",
      "         2.9233e-01,  4.5697e-01,  6.9574e-01, -6.1304e-01, -5.9497e-01,\n",
      "         1.6857e-01,  4.1672e-01,  4.8422e-02,  4.2479e-01, -2.8655e-01,\n",
      "        -1.6701e-01,  9.9271e-02, -6.5356e-02,  4.2338e-01, -1.3058e-01,\n",
      "        -3.7394e-03, -8.1774e-02, -8.1623e-02,  8.6240e-02,  8.6483e-02,\n",
      "        -8.6864e-01,  2.7941e-01, -3.2422e-01, -3.1130e-01,  5.8784e-01,\n",
      "         9.4711e-02, -2.0726e-01, -3.6508e-01,  7.5609e-02, -9.0123e-01,\n",
      "         8.3788e-01, -1.4727e-01, -1.4472e-01,  4.0994e-01,  4.6384e-01,\n",
      "         1.3198e-01,  7.3628e-01, -2.1156e-01,  1.4486e-03,  6.4053e-01,\n",
      "         1.3718e-01, -5.8394e-02,  1.8562e-01,  2.2558e-01,  6.8480e-01,\n",
      "         4.9667e-01,  6.8147e-01,  7.2069e-01, -1.1113e-01,  3.4883e-01,\n",
      "        -6.7245e-01,  2.5415e-01,  2.6150e-01, -2.2076e-01, -8.5221e-01,\n",
      "         3.0382e-01,  4.6686e-01, -4.8040e-01, -3.6181e-01, -3.2198e-01,\n",
      "        -7.0319e-01, -1.0532e+00,  5.6308e-01,  3.2096e-01,  3.7040e-01,\n",
      "        -1.0348e-01,  1.8833e-01, -5.5544e-03,  7.4825e-01, -7.9143e-01,\n",
      "        -2.3073e-01, -3.5965e-01, -1.2498e-01, -1.1993e-01,  9.1093e-02,\n",
      "         8.8660e-02, -3.0148e-01,  2.7432e-01, -1.7573e-01, -3.1268e-01,\n",
      "        -2.4671e-01,  3.5752e-01, -3.3529e-02,  1.9741e-02, -4.4740e-01,\n",
      "        -4.0709e-01,  1.3870e+00,  4.2135e-01,  2.2320e-01,  4.1661e-01,\n",
      "        -9.4495e-02,  7.0101e-01,  3.0496e-01, -3.5386e-01,  1.4744e-01,\n",
      "         4.0640e-01,  2.1345e-01,  6.0065e-01,  5.5833e-01,  1.4555e-01,\n",
      "         3.9360e-01, -1.6167e-02,  4.8321e-01,  5.6634e-01,  5.3667e-01,\n",
      "         1.3718e-01, -2.9431e-01, -2.8097e-01,  8.2949e-01,  4.4219e-01,\n",
      "        -3.8308e-01,  3.3198e-01, -3.0106e-01, -1.4564e-01,  4.9073e-01,\n",
      "         6.7923e-01, -6.4548e-01, -1.9663e-01, -1.9501e-02,  2.3544e-02,\n",
      "         1.5547e-02,  5.8681e-01,  3.1208e-01,  3.5604e-02,  2.3218e-01,\n",
      "        -7.2947e-01, -3.0371e-01,  2.9267e-01, -9.3698e-01,  5.7429e-01,\n",
      "        -5.4629e-01,  7.3198e-01, -8.2737e-01,  7.3887e-01, -1.8886e-01,\n",
      "         1.7766e-01,  4.7530e-01, -7.6629e-01, -2.0449e-01, -2.1478e-01,\n",
      "        -2.0553e-02,  3.8217e-01, -2.7026e-01, -2.4869e-01, -6.9473e-02,\n",
      "        -1.9237e-01,  7.8231e-01,  2.8227e-01,  2.5193e-01, -1.2265e-01,\n",
      "         1.3030e-01, -3.9701e-01, -4.4863e-01,  6.8303e-01,  6.0472e-01,\n",
      "        -8.6355e-01,  1.7481e-01, -1.0260e-01, -1.8385e-01,  1.9073e-01,\n",
      "         5.2710e-01, -7.7151e-01,  4.6560e-01,  2.3571e-01, -2.4083e-01,\n",
      "        -2.7851e-01,  2.6116e-01, -4.8455e-01, -5.6011e-01,  2.8244e-01,\n",
      "         7.9717e-01,  1.0085e-02,  1.6930e-01,  5.3290e-02,  3.0302e-01,\n",
      "         5.0247e-01,  1.5811e-01, -2.4355e-01,  2.9496e-01, -2.9124e-01,\n",
      "         3.0045e-01, -4.6658e-01, -2.8872e-01,  5.1095e-01,  3.0687e-01,\n",
      "         2.2239e-01, -2.2828e-01,  3.8841e-01, -2.3812e-01,  2.3414e-01,\n",
      "        -7.3673e-01, -5.1072e-01, -2.5295e-01,  1.3606e-01,  3.7154e-01,\n",
      "         2.7335e-01, -3.1028e-01, -7.3657e-01,  4.4031e-01, -2.0341e-01,\n",
      "         1.6027e-01,  4.5352e-01, -5.0094e-02])\n",
      "Embedding for 'bank' in sentence 2: tensor([ 3.0805e-01, -5.0289e-01, -7.1262e-01,  7.6031e-01,  2.4884e-01,\n",
      "        -4.3137e-01, -2.0040e-01,  6.7196e-01,  2.7493e-02, -6.1076e-01,\n",
      "        -1.3500e-01, -1.2207e-01, -4.3199e-01,  2.1680e-02, -5.1839e-01,\n",
      "         4.0978e-01, -4.6856e-02, -3.0052e-01,  2.6342e-01, -1.0299e-01,\n",
      "         4.2932e-02, -3.1060e-01, -6.3818e-01,  3.2220e-01,  4.7629e-01,\n",
      "         2.7052e-01,  2.5409e-01, -1.2060e-01, -6.2286e-01,  2.5875e-01,\n",
      "         1.8869e-01,  5.9632e-01, -1.2332e-01, -5.1908e-01, -2.8664e-01,\n",
      "         3.2440e-01,  1.3782e-01, -3.0374e-02, -8.6321e-01,  4.6259e-01,\n",
      "        -1.1555e+00, -4.5584e-01,  3.1995e-01,  2.2986e-01,  8.1595e-01,\n",
      "        -5.8041e-01,  5.1974e-01, -3.1748e-01,  1.4955e-01, -6.5844e-01,\n",
      "        -4.6035e-01,  5.1712e-01, -2.6741e-01, -3.4860e-01,  1.2507e-01,\n",
      "         1.3501e+00, -2.3182e-01, -2.9527e-01, -2.6664e-02,  9.0901e-02,\n",
      "         3.4737e-02, -6.2653e-02,  3.7076e-01, -2.3562e-01, -3.9354e-01,\n",
      "         2.3364e-01,  1.9642e-01,  6.8836e-01,  3.9060e-02,  5.5483e-01,\n",
      "        -4.0763e-01, -2.0189e-01, -6.9198e-02, -1.0598e+00, -3.0124e-01,\n",
      "        -3.5010e-01, -1.2803e-01,  3.8439e-01,  2.0512e-01, -5.7446e-01,\n",
      "         4.8744e-01,  8.9397e-01, -6.8436e-01,  1.1263e+00,  1.8688e-01,\n",
      "         2.2970e-01, -8.4325e-02,  3.0994e-02, -1.2147e+00,  6.9401e-01,\n",
      "        -3.7950e-01,  2.9050e-01,  1.3625e-01, -2.5267e-01,  3.1647e-01,\n",
      "         2.7736e-01,  6.3147e-02, -1.0514e-01,  6.1273e-01,  1.3197e+00,\n",
      "         5.3605e-02, -3.7698e-01,  1.8544e-02,  2.7060e-01, -1.4266e+00,\n",
      "        -3.8618e-01,  1.9149e-02, -6.7627e-01, -5.0767e-01,  6.7623e-01,\n",
      "         6.9909e-01,  5.1211e-02,  3.8937e-01,  4.4009e-01,  2.4744e-01,\n",
      "         9.8141e-01,  1.3099e-01,  5.9353e-02, -4.4021e-01,  1.1157e-01,\n",
      "         2.9937e-01, -3.2133e-01,  5.5058e-01,  6.4182e-01,  1.1284e-01,\n",
      "         6.1395e-01, -3.1705e-01,  5.4412e-01, -4.8147e-01, -4.3814e-01,\n",
      "         1.2336e+00,  1.0088e+00,  1.7431e-01, -1.1969e+00,  1.4132e-01,\n",
      "         5.6291e-01,  3.5822e-01, -5.0976e-01, -1.8364e+00,  5.1024e-01,\n",
      "         4.5031e-01, -3.3429e-01,  5.0731e-01, -2.9543e-01,  3.7106e-01,\n",
      "         1.6636e-01,  3.8138e-01, -4.8933e-01,  3.0599e-01,  8.2161e-01,\n",
      "         4.8841e-01, -6.6945e-01, -4.3877e-01, -3.6337e-01, -9.9641e-02,\n",
      "        -7.4862e-02,  1.6672e-01,  2.2670e-01, -2.9477e-01,  6.7434e-01,\n",
      "         8.5536e-01, -1.8934e-01, -2.4221e-01, -9.8529e-01,  7.0023e-02,\n",
      "         3.8478e-01, -1.8803e-01,  1.3192e+00, -2.3432e-01,  6.5685e-01,\n",
      "        -4.0678e-02,  5.5181e-01,  1.0845e+00, -5.5465e-01, -3.3929e-01,\n",
      "        -8.1930e-01, -3.1686e-01,  9.1590e-02, -1.7531e-01,  4.4987e-01,\n",
      "        -4.3679e-01, -3.1283e-01,  2.9556e-01, -3.9294e-01,  3.1370e-01,\n",
      "        -6.6278e-01,  3.7381e-01, -4.9136e-01, -3.1247e-01, -1.2219e-01,\n",
      "         2.5881e-01, -9.2079e-02, -9.4155e-01,  1.6979e-02, -9.4916e-02,\n",
      "        -1.1375e+00, -5.2261e-01, -1.0032e+00, -5.4685e-01, -2.9206e-01,\n",
      "         2.3236e-01, -6.5463e-01,  3.7885e-01,  6.0897e-01, -8.1060e-01,\n",
      "        -2.2151e-01,  1.2064e-02, -4.5579e-01, -4.3081e-01,  2.3055e-01,\n",
      "        -1.5774e+00,  6.5483e-01,  3.1965e-01,  2.3752e-01, -2.3532e-01,\n",
      "        -8.7878e-02, -7.4321e-02, -7.6961e-02, -3.3137e-01, -3.8177e-01,\n",
      "        -3.5084e-01,  4.2007e-01, -6.9694e-01,  2.7140e-01,  1.4875e-01,\n",
      "         1.4975e+00,  7.0052e-01, -4.4869e-01, -4.4575e-01,  1.2735e+00,\n",
      "         1.1802e-01, -8.5788e-01,  9.9686e-01, -1.2750e-01, -8.5776e-01,\n",
      "        -5.6050e-01, -1.1193e+00, -2.8468e-01,  3.3695e-01,  9.0362e-02,\n",
      "         2.4242e-01, -8.5962e-01,  8.3741e-01, -3.7088e-01,  6.0731e-01,\n",
      "        -1.7406e-01, -2.6211e-01,  6.8336e-01, -7.8529e-01, -5.2524e-01,\n",
      "        -8.0496e-02, -7.7961e-01,  3.0701e-02, -7.5259e-01, -2.3697e-01,\n",
      "        -5.2549e-01,  6.0104e-02, -5.0330e-01, -4.9360e-01,  4.6014e-01,\n",
      "         2.8419e-01, -2.8406e-01,  1.4287e-01,  1.1194e-01, -5.8316e-01,\n",
      "        -5.3394e-01,  7.4929e-01,  1.8384e-01, -1.6299e-01,  1.2609e-01,\n",
      "        -1.6209e-01, -5.1031e-01, -5.8329e-02,  9.1035e-01, -7.8741e-01,\n",
      "        -7.3293e-01,  4.8164e-01, -6.7586e-01,  9.7433e-02, -4.4746e-01,\n",
      "         5.4107e-01,  1.2147e-01, -1.3283e+00, -2.1411e-01,  2.2694e-02,\n",
      "        -2.7016e-01, -4.5008e-01,  6.2392e-01, -5.3675e-01, -2.2965e-01,\n",
      "         9.4939e-02, -7.3372e-02, -6.1442e-02, -4.7358e-01, -5.5343e-01,\n",
      "        -1.4745e-01, -1.2756e-01,  1.5063e-01,  9.8020e-01, -2.8839e-01,\n",
      "        -1.5344e-01,  4.4658e-01,  1.6120e-01,  1.4932e-01, -5.8949e-01,\n",
      "         5.8967e-02, -1.1302e-01, -6.2239e-01, -4.1313e+00,  7.6491e-01,\n",
      "         4.0141e-01, -1.7030e-01,  4.1546e-01,  2.0475e-01, -3.6498e-01,\n",
      "         1.1901e-01,  3.6253e-02,  1.8695e-02, -4.0023e-01,  1.6181e-01,\n",
      "         4.8859e-01, -1.1051e-01,  4.2301e-01, -7.1971e-01,  7.5306e-01,\n",
      "        -1.3202e-01, -2.7076e-01,  4.8734e-01, -1.4806e-01, -3.4320e-01,\n",
      "        -2.1699e-01, -8.2310e-01,  2.6691e-01,  4.1034e-01,  1.6512e-01,\n",
      "        -3.2198e-01, -1.3008e-01, -1.2060e-01,  7.3695e-02, -5.2525e-01,\n",
      "        -1.0665e-01,  3.2997e-01,  4.9953e-01,  3.4583e-01,  4.3746e-01,\n",
      "        -1.0988e-01,  1.5892e-01,  9.2758e-01, -4.7974e-01, -2.5368e-02,\n",
      "         5.3200e-02, -1.5738e-02,  1.1506e+00, -2.6304e-01, -3.1447e-01,\n",
      "        -6.9984e-03,  6.5319e-01,  3.4811e-01,  3.9949e-02, -5.1473e-01,\n",
      "        -5.6276e-02,  7.7851e-02, -1.9609e-01,  3.1304e-01,  7.0745e-01,\n",
      "        -3.7159e-01,  1.8490e-01, -7.1596e-01,  5.6763e-01,  3.0148e-01,\n",
      "        -3.9998e-01,  6.8911e-01,  5.2331e-01, -1.0298e+00, -9.2992e-01,\n",
      "        -5.5070e-02,  8.3270e-02,  4.4513e-01, -1.9152e-02,  1.4348e-01,\n",
      "        -1.0505e+00, -1.0385e+00, -7.1493e-01,  1.5872e-01,  8.4043e-01,\n",
      "        -1.4922e-01,  4.5312e-01, -6.5943e-01, -2.5371e-01, -1.8668e-01,\n",
      "         4.2595e-01, -1.3399e-01,  4.0139e-01, -9.8272e-01, -1.8095e-01,\n",
      "        -1.4522e-01, -3.6239e-01, -8.2444e-01, -4.3388e-01,  3.8142e-01,\n",
      "        -2.1186e-01,  2.0542e-01, -4.8436e-01,  5.1002e-01,  4.3563e-01,\n",
      "        -8.8589e-01,  4.5409e-02,  5.3882e-01,  5.0292e-01,  4.3329e-01,\n",
      "         6.7201e-01,  4.8607e-01, -3.9297e-02, -3.1004e-02, -1.2039e+00,\n",
      "        -3.4205e-01,  4.2005e-02,  1.3363e-01,  1.5387e-04, -6.3261e-01,\n",
      "         1.2006e+00,  1.2471e-01, -1.0385e+00,  2.7272e-01,  1.3045e+00,\n",
      "         2.5327e-01, -4.0479e-01,  3.9774e-01,  4.3061e-01,  7.1953e-01,\n",
      "        -1.4667e-01, -1.5516e-01,  4.7211e-01, -4.2222e-01,  5.9258e-01,\n",
      "        -5.3297e-01, -2.8391e-01,  4.3543e-03,  2.7178e-01, -1.9914e-01,\n",
      "         3.5137e-01,  1.9920e-01, -2.0203e-01, -5.2931e-02, -5.1129e-01,\n",
      "        -1.5419e+00,  5.5429e-01,  1.1412e-01, -3.8431e-01,  9.2062e-01,\n",
      "         5.5059e-01, -1.2722e-01, -1.3442e-01,  2.4803e-01,  2.5655e-01,\n",
      "        -8.2155e-01,  1.2172e-01,  6.6191e-02, -3.4432e-01,  4.2032e-01,\n",
      "         7.7338e-03,  1.0199e-01,  3.4118e-01, -5.9360e-01,  9.3703e-01,\n",
      "        -4.5913e-02, -3.5192e-01, -2.7084e-01, -4.9577e-01,  6.0616e-01,\n",
      "         4.3953e-01,  4.5250e-01, -6.8591e-01,  5.9476e-01,  7.6318e-02,\n",
      "        -2.9433e-01, -5.1561e-01,  4.8986e-01,  1.7773e-01,  2.9197e-02,\n",
      "        -6.8595e-02,  6.5731e-01, -6.9742e-01,  1.1145e+00, -1.9829e-01,\n",
      "        -3.2918e-01, -8.0707e-01, -1.0144e+00, -6.3801e-01, -5.1956e-01,\n",
      "        -4.1088e-01, -1.4855e-01,  3.4762e-01, -7.1138e-02,  8.9045e-02,\n",
      "        -1.2229e-01, -1.6565e-01, -1.8384e-03, -3.9152e-03,  5.3878e-01,\n",
      "        -5.4456e-01, -2.2016e-01, -2.6465e-01, -4.6242e-01,  2.2879e-01,\n",
      "         8.8766e-01,  1.6613e-02, -1.9476e-01,  3.5929e-01, -1.9758e-01,\n",
      "        -5.1311e-01, -1.5642e-01, -2.2841e-01, -7.5132e-01,  2.0842e-02,\n",
      "         5.1273e-01, -2.2545e-01,  1.2806e-01, -4.4603e-01, -6.9007e-01,\n",
      "        -4.7527e-02, -1.0462e+00,  1.7765e-01, -2.4920e-01,  4.2534e-01,\n",
      "        -5.5860e-01, -2.1373e-01,  3.5874e-01, -3.4559e-01, -1.6268e-02,\n",
      "         5.0028e-02, -9.7018e-02, -9.8330e-01, -5.3180e-02,  9.4807e-02,\n",
      "         5.5309e-01, -4.9502e-01, -2.3216e-01, -2.5849e-01, -7.5884e-01,\n",
      "         4.1183e-01,  1.7111e-01,  7.4673e-01,  2.7603e-01, -2.2661e-01,\n",
      "        -7.8265e-01, -3.3779e-01,  2.7219e-02,  1.1570e-01, -1.7693e-01,\n",
      "        -4.9998e-02, -5.2041e-01,  2.0458e-01, -4.4457e-01, -6.1645e-01,\n",
      "         1.2887e+00, -4.9753e-01, -1.4465e-01,  4.5267e-01,  7.2818e-01,\n",
      "         1.0967e-01, -2.2386e-01, -2.9536e-01,  3.0228e-02, -3.2935e-01,\n",
      "        -1.4355e-01,  3.2984e-01,  1.3914e+00, -6.5608e-02, -5.7129e-01,\n",
      "        -1.7052e-01,  5.2792e-01,  6.8277e-01, -4.8324e-01,  2.8189e-01,\n",
      "        -1.1136e-01,  2.7701e-01, -2.8569e-03,  1.7002e-01,  1.3577e-01,\n",
      "         1.2978e-01,  2.1820e-02,  1.3557e-01,  2.1059e-01, -8.5184e-02,\n",
      "        -1.0707e+00,  1.6355e-01, -5.5906e-03, -6.0163e-01,  7.2935e-01,\n",
      "         2.9047e-01,  3.8087e-02, -7.0257e-01,  2.4761e-01, -1.8694e-01,\n",
      "         9.9496e-01, -1.2076e-01, -4.7042e-02,  5.6472e-01,  9.5521e-02,\n",
      "         4.7394e-01,  4.7945e-01,  6.7880e-01,  8.8459e-02,  8.0028e-01,\n",
      "        -8.0319e-02, -6.9364e-01,  2.3382e-01, -3.4839e-01, -1.4777e-01,\n",
      "         7.9820e-01,  5.1635e-01,  1.2903e-01, -4.7770e-01, -2.1427e-01,\n",
      "        -3.5193e-01,  1.9307e-01,  8.5906e-01,  9.0074e-02, -3.8723e-01,\n",
      "         3.4239e-01,  6.7534e-01, -4.3000e-01,  2.1930e-01, -5.1725e-01,\n",
      "        -2.8977e-02, -3.1816e-01,  5.2176e-01, -1.3738e-01, -3.2043e-03,\n",
      "        -2.0784e-01,  3.7294e-01,  5.5548e-01,  3.3446e-01, -5.1523e-01,\n",
      "        -6.4316e-01,  3.0679e-01,  6.9944e-01, -6.1133e-01,  6.1560e-01,\n",
      "         1.6191e-01, -1.0942e-01,  1.5970e-01, -6.9012e-02, -1.5843e-01,\n",
      "         3.2031e-01,  1.1904e-01, -6.1320e-01,  1.5598e-01, -1.3396e-01,\n",
      "         1.2658e-01,  2.7602e-01,  4.9159e-01, -6.8290e-01,  6.5974e-01,\n",
      "        -1.8123e-02,  6.4631e-01,  1.6088e-01,  2.2389e-01, -2.3916e-01,\n",
      "         1.5069e-01,  3.1170e-01,  5.3487e-01,  1.2683e+00,  3.4233e-01,\n",
      "        -1.7172e-01,  5.7902e-01,  1.1165e+00,  8.5376e-01,  1.0379e+00,\n",
      "        -2.2847e-01, -9.1062e-01, -2.4160e-01,  9.4829e-01, -1.5285e-01,\n",
      "        -7.1943e-01, -4.4803e-01,  3.8376e-01, -7.7606e-02, -3.6812e-02,\n",
      "         6.7471e-01, -3.9583e-01,  1.3660e-01,  6.3572e-01, -3.6201e-01,\n",
      "        -4.1594e-01,  1.7286e-01,  4.2029e-01,  1.2623e-02,  5.1827e-01,\n",
      "        -9.9477e-01, -3.8511e-01,  2.2037e-01, -5.0315e-01,  2.6988e-01,\n",
      "         2.4313e-01, -4.7937e-01, -5.1172e-01, -6.5766e-01, -3.7452e-01,\n",
      "         4.8168e-01, -8.8908e-02, -7.1443e-02,  2.6668e-02, -2.0357e-01,\n",
      "         4.4324e-01,  8.1707e-01,  1.0916e-01, -2.7278e-01, -3.3110e-01,\n",
      "        -7.6944e-01,  5.1109e-01,  7.1769e-02,  3.3870e-01, -2.6919e-01,\n",
      "        -6.4908e-01, -1.9789e-01,  2.7184e-01,  1.0754e+00,  7.5589e-01,\n",
      "        -7.5241e-01,  7.1542e-01, -2.4017e-02, -8.0648e-01,  2.9311e-01,\n",
      "         3.7514e-01,  2.5521e-01,  6.9205e-01, -5.8085e-01,  8.1448e-02,\n",
      "        -3.7781e-01,  5.7581e-01, -8.8372e-01, -5.2900e-01,  7.0205e-01,\n",
      "         7.8684e-01,  7.2102e-02,  2.2282e-01, -5.0638e-01,  3.6975e-01,\n",
      "        -2.5054e-01, -9.0045e-02, -2.2062e-01,  1.6270e-01, -8.1743e-01,\n",
      "        -3.1187e-01, -4.2528e-01, -7.6323e-01,  7.4418e-01,  4.6071e-01,\n",
      "         9.2637e-01,  7.2146e-01,  9.1599e-01, -3.3185e-01,  1.4425e-01,\n",
      "         7.6300e-02,  2.0732e-01, -1.5597e+00, -2.7509e-01,  2.1308e-01,\n",
      "         1.7805e-01, -2.6255e-02, -7.7412e-01,  5.2445e-02,  2.0471e-01,\n",
      "        -3.6424e-01, -5.0539e-01,  2.5425e-01])\n",
      "Cosine similarity between the two embeddings: 0.5922409\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity 계산\n",
    "\n",
    "similarity = cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2)\n",
    "print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
